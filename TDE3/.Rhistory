str(beans_data)
class_factor = factor(beans_data$Class)
beans_data$Class = class_factor
set.seed(1)
partitions = createFolds(beans_data$Class, k = 3)
results = c(0,0,0)
for(partition in 1:3) {
training_indexes = unlist(partitions[-partition])
test_indexes     = partitions[[partition]]
training_data    = beans_data[training_indexes,-17]
training_labels  = beans_data[training_indexes,17]
test_data        = beans_data[test_indexes,-17]
test_labels      = beans_data[test_indexes,17]
## Seleção de caracteristicas
correlation_matrix = cor(training_data)
strong_correlations = findCorrelation(correlation_matrix, cutoff=0.95)
if(length(strong_correlations) > 0) {
training_data[,strong_correlations] = NULL
test_data[,strong_correlations] = NULL
}
gmm.model = MclustDA(training_data, training_labels, modelNames = c('VII'), verbose = FALSE)
gmm.predcition = predict(gmm.model, test_data)
results[partition] = confusionMatrix(gmm.predict$classification, test_labels)$overall[1]
}
mean(results)
rm(list=ls())
library(caret)
library(mclust)
beans_data = read.csv("Dry_Bean_Dataset.csv",header=T, sep=";", na.strings="?", dec=",")
str(beans_data)
class_factor = factor(beans_data$Class)
beans_data$Class = class_factor
set.seed(1)
partitions = createFolds(beans_data$Class, k = 3)
results = c(0,0,0)
for(partition in 1:3) {
training_indexes = unlist(partitions[-partition])
test_indexes     = partitions[[partition]]
training_data    = beans_data[training_indexes,-17]
training_labels  = beans_data[training_indexes,17]
test_data        = beans_data[test_indexes,-17]
test_labels      = beans_data[test_indexes,17]
## Seleção de caracteristicas
correlation_matrix = cor(training_data)
strong_correlations = findCorrelation(correlation_matrix, cutoff=0.95)
if(length(strong_correlations) > 0) {
training_data[,strong_correlations] = NULL
test_data[,strong_correlations] = NULL
}
gmm.model = MclustDA(training_data, training_labels, modelNames = c('VII'), verbose = FALSE)
gmm.predcit = predict(gmm.model, test_data)
results[partition] = confusionMatrix(gmm.predict$classification, test_labels)$overall[1]
}
mean(results)
rm(list=ls())
library(caret)
library(mclust)
beans_data = read.csv("Dry_Bean_Dataset.csv",header=T, sep=";", na.strings="?", dec=",")
str(beans_data)
class_factor = factor(beans_data$Class)
beans_data$Class = class_factor
set.seed(1)
partitions = createFolds(beans_data$Class, k = 3)
results = c(0,0,0)
for(partition in 1:3) {
training_indexes = unlist(partitions[-partition])
test_indexes     = partitions[[partition]]
training_data    = beans_data[training_indexes,-17]
training_labels  = beans_data[training_indexes,17]
test_data        = beans_data[test_indexes,-17]
test_labels      = beans_data[test_indexes,17]
## Seleção de caracteristicas
correlation_matrix = cor(training_data)
strong_correlations = findCorrelation(correlation_matrix, cutoff=0.95)
if(length(strong_correlations) > 0) {
training_data[,strong_correlations] = NULL
test_data[,strong_correlations] = NULL
}
gmm.model = MclustDA(training_data, training_labels, modelNames = c('VII'), verbose = FALSE)
gmm.predict = predict(gmm.model, test_data)
results[partition] = confusionMatrix(gmm.predict$classification, test_labels)$overall[1]
}
mean(results)
rm(list=ls())
library(caret)
library(mclust)
beans_data = read.csv("Dry_Bean_Dataset.csv",header=T, sep=";", na.strings="?", dec=",")
str(beans_data)
class_factor = factor(beans_data$Class)
beans_data$Class = class_factor
set.seed(1)
partitions = createFolds(beans_data$Class, k = 3)
results = c(0,0,0)
for(partition in 1:3) {
training_indexes = unlist(partitions[-partition])
test_indexes     = partitions[[partition]]
training_data    = beans_data[training_indexes,-17]
training_labels  = beans_data[training_indexes,17]
test_data        = beans_data[test_indexes,-17]
test_labels      = beans_data[test_indexes,17]
## Seleção de caracteristicas
correlation_matrix = cor(training_data)
strong_correlations = findCorrelation(correlation_matrix, cutoff=0.95)
if(length(strong_correlations) > 0) {
training_data[,strong_correlations] = NULL
test_data[,strong_correlations] = NULL
}
gmm.model = MclustDA(training_data, training_labels, modelNames = c('VII'), verbose = FALSE)
gmm.predict = predict(gmm.model, test_data)
results[partition] = confusionMatrix(gmm.predict$classification, test_labels)$overall[1]
}
mean(results)
rm(list=ls())
library(caret)
library(mclust)
beans_data = read.csv("Dry_Bean_Dataset.csv",header=T, sep=";", na.strings="?", dec=",")
str(beans_data)
class_factor = factor(beans_data$Class)
beans_data$Class = class_factor
set.seed(1)
partitions = createFolds(beans_data$Class, k = 3)
results = c(0,0,0)
for(partition in 1:3) {
training_indexes = unlist(partitions[-partition])
test_indexes     = partitions[[partition]]
training_data    = beans_data[training_indexes,-17]
training_labels  = beans_data[training_indexes,17]
test_data        = beans_data[test_indexes,-17]
test_labels      = beans_data[test_indexes,17]
## Normalização por Score-Z
normalization_parameters = preProcess(training_data, method = c("center","scale"))
training_data            = predict(normalization_parameters, training_data)
test_data                = predict(normalization_parameters, test_data)
rm(normalization_parameters)
gmm.model = MclustDA(training_data, training_labels, modelNames = c('VII'), verbose = FALSE)
gmm.predict = predict(gmm.model, test_data)
results[partition] = confusionMatrix(gmm.predict$classification, test_labels)$overall[1]
}
mean(results)
rm(list=ls())
library(caret)
library(mclust)
beans_data = read.csv("Dry_Bean_Dataset.csv",header=T, sep=";", na.strings="?", dec=",")
str(beans_data)
class_factor = factor(beans_data$Class)
beans_data$Class = class_factor
set.seed(1)
partitions = createFolds(beans_data$Class, k = 3)
results = c(0,0,0)
for(partition in 1:3) {
training_indexes = unlist(partitions[-partition])
test_indexes     = partitions[[partition]]
training_data    = beans_data[training_indexes,-17]
training_labels  = beans_data[training_indexes,17]
test_data        = beans_data[test_indexes,-17]
test_labels      = beans_data[test_indexes,17]
gmm.model = MclustDA(training_data, training_labels, modelNames = c('VII'), verbose = FALSE)
gmm.predict = predict(gmm.model, test_data)
results[partition] = confusionMatrix(gmm.predict$classification, test_labels)$overall[1]
}
mean(results)
confusionMatrix(gmm.predict$classification, test_labels)
confusion_matrix = confusionMatrix(gmm.predict$classification, test_labels)
confusion_matrix$overall[2]
View(confusion_matrix)
View(confusion_matrix)
rm(list=ls())
library(caret)
library(mclust)
beans_data = read.csv("Dry_Bean_Dataset.csv",header=T, sep=";", na.strings="?", dec=",")
str(beans_data)
class_factor = factor(beans_data$Class)
beans_data$Class = class_factor
set.seed(1)
partitions = createFolds(beans_data$Class, k = 3)
results = c(0,0,0)
for(partition in 1:3) {
training_indexes = unlist(partitions[-partition])
test_indexes     = partitions[[partition]]
training_data    = beans_data[training_indexes,-17]
training_labels  = beans_data[training_indexes,17]
test_data        = beans_data[test_indexes,-17]
test_labels      = beans_data[test_indexes,17]
## Normalização por Score-Z
normalization_parameters = preProcess(training_data, method = c("center","scale"))
training_data            = predict(normalization_parameters, training_data)
test_data                = predict(normalization_parameters, test_data)
rm(normalization_parameters)
## Seleção de caracteristicas
correlation_matrix = cor(training_data)
strong_correlations = findCorrelation(correlation_matrix, cutoff=0.95)
if(length(strong_correlations) > 0) {
training_data[,strong_correlations] = NULL
test_data[,strong_correlations] = NULL
}
## Treinamento de dados utilizando modelo de misturas gaussianas de covariância esférica
gmm.model = MclustDA(training_data, training_labels, modelNames = c("VII"), verbose = FALSE)
## Predição dos dados
gmm.predict = predict(gmm.model, test_data)
results[partition] = confusionMatrix(gmm.predict$classification, test_labels)$overall[1]
}
mean(results)
## Utilizando o modelo de misturas gaussianas de covariância esférica, se obteve acurácia
## de 90.14% para os dados de teste.
rm(list=ls())
library(caret)
library(mclust)
beans_data = read.csv("Dry_Bean_Dataset.csv",header=T, sep=";", na.strings="?", dec=",")
str(beans_data)
class_factor = factor(beans_data$Class)
beans_data$Class = class_factor
set.seed(1)
partitions = createFolds(beans_data$Class, k = 3)
results = c(0,0,0)
for(partition in 1:3) {
training_indexes = unlist(partitions[-partition])
test_indexes     = partitions[[partition]]
training_data    = beans_data[training_indexes,-17]
training_labels  = beans_data[training_indexes,17]
test_data        = beans_data[test_indexes,-17]
test_labels      = beans_data[test_indexes,17]
## Normalização por Score-Z
normalization_parameters = preProcess(training_data, method = c("center","scale"))
training_data            = predict(normalization_parameters, training_data)
test_data                = predict(normalization_parameters, test_data)
rm(normalization_parameters)
## Seleção de caracteristicas
correlation_matrix = cor(training_data)
strong_correlations = findCorrelation(correlation_matrix, cutoff=0.95)
if(length(strong_correlations) > 0) {
training_data[,strong_correlations] = NULL
test_data[,strong_correlations] = NULL
}
## Treinamento de dados utilizando modelo de misturas gaussianas de covariância esférica
gmm.model = MclustDA(training_data, training_labels, modelNames = c("VII"), verbose = FALSE)
## Predição dos dados
gmm.predict = predict(gmm.model, test_data)
confusion_matrix = confusionMatrix(gmm.predict$classification, test_labels)
confusion_matrix
results[partition] = confusion_matrix$overall[1]
}
mean(results)
## Utilizando o modelo de misturas gaussianas de covariância esférica, se obteve acurácia
## de 90.14% para os dados de teste.
rm(list=ls())
library(caret)
library(mclust)
beans_data = read.csv("Dry_Bean_Dataset.csv",header=T, sep=";", na.strings="?", dec=",")
str(beans_data)
class_factor = factor(beans_data$Class)
beans_data$Class = class_factor
set.seed(1)
partitions = createFolds(beans_data$Class, k = 3)
results = c(0,0,0)
for(partition in 1:3) {
training_indexes = unlist(partitions[-partition])
test_indexes     = partitions[[partition]]
training_data    = beans_data[training_indexes,-17]
training_labels  = beans_data[training_indexes,17]
test_data        = beans_data[test_indexes,-17]
test_labels      = beans_data[test_indexes,17]
## Normalização por Score-Z
normalization_parameters = preProcess(training_data, method = c("center","scale"))
training_data            = predict(normalization_parameters, training_data)
test_data                = predict(normalization_parameters, test_data)
rm(normalization_parameters)
## Seleção de caracteristicas
correlation_matrix = cor(training_data)
strong_correlations = findCorrelation(correlation_matrix, cutoff=0.95)
if(length(strong_correlations) > 0) {
training_data[,strong_correlations] = NULL
test_data[,strong_correlations] = NULL
}
## Treinamento de dados utilizando modelo de misturas gaussianas de covariância esférica
gmm.model = MclustDA(training_data, training_labels, modelNames = c("VII"), verbose = FALSE)
## Predição dos dados
gmm.predict = predict(gmm.model, test_data)
confusion_matrix = confusionMatrix(gmm.predict$classification, test_labels)
print(confusion_matrix)
results[partition] = confusion_matrix$overall[1]
}
mean(results)
## Utilizando o modelo de misturas gaussianas de covariância esférica, se obteve acurácia
## de 90.14% para os dados de teste.
source("C:/Users/Leonardo Martelli/source/repos/Trabalho-1-Comp-Aplicada/TDE3/Dry_Bean_Dataset_exp4.R")
rm(list=ls())
library(caret)
library(mclust)
beans_data = read.csv("Dry_Bean_Dataset.csv",header=T, sep=";", na.strings="?", dec=",")
str(beans_data)
class_factor = factor(beans_data$Class)
beans_data$Class = class_factor
set.seed(1)
partitions = createFolds(beans_data$Class, k = 3)
results = c(0,0,0)
for(partition in 1:3) {
training_indexes = unlist(partitions[-partition])
test_indexes     = partitions[[partition]]
training_data    = beans_data[training_indexes,-17]
training_labels  = beans_data[training_indexes,17]
test_data        = beans_data[test_indexes,-17]
test_labels      = beans_data[test_indexes,17]
## Seleção de caracteristicas
correlation_matrix = cor(training_data)
strong_correlations = findCorrelation(correlation_matrix, cutoff=0.95)
if(length(strong_correlations) > 0) {
training_data[,strong_correlations] = NULL
test_data[,strong_correlations] = NULL
}
gmm.model = MclustDA(training_data, training_labels, modelNames = c('VII'), verbose = FALSE)
gmm.predict = predict(gmm.model, test_data)
confusion_matrix = confusionMatrix(gmm.predict$classification, test_labels)
print(confusion_matrix)
results[partition] = confusion_matrix$overall[1]
}
mean(results)
rm(list=ls())
library(caret)
library(mclust)
beans_data = read.csv("Dry_Bean_Dataset.csv",header=T, sep=";", na.strings="?", dec=",")
str(beans_data)
class_factor = factor(beans_data$Class)
beans_data$Class = class_factor
set.seed(1)
partitions = createFolds(beans_data$Class, k = 3)
results = c(0,0,0)
for(partition in 1:3) {
training_indexes = unlist(partitions[-partition])
test_indexes     = partitions[[partition]]
training_data    = beans_data[training_indexes,-17]
training_labels  = beans_data[training_indexes,17]
test_data        = beans_data[test_indexes,-17]
test_labels      = beans_data[test_indexes,17]
## Seleção de caracteristicas
correlation_matrix = cor(training_data)
strong_correlations = findCorrelation(correlation_matrix, cutoff=0.95)
if(length(strong_correlations) > 0) {
training_data[,strong_correlations] = NULL
test_data[,strong_correlations] = NULL
}
gmm.model = MclustDA(training_data, training_labels, modelNames = c('VII'), verbose = FALSE)
gmm.predict = predict(gmm.model, test_data)
confusion_matrix = confusionMatrix(gmm.predict$classification, test_labels)
print(confusion_matrix)
results[partition] = confusion_matrix$overall[1]
}
mean(results)
rm(list=ls())
library(caret)
library(mclust)
beans_data = read.csv("Dry_Bean_Dataset.csv",header=T, sep=";", na.strings="?", dec=",")
str(beans_data)
class_factor = factor(beans_data$Class)
beans_data$Class = class_factor
set.seed(1)
partitions = createFolds(beans_data$Class, k = 3)
results = c(0,0,0)
for(partition in 1:3) {
training_indexes = unlist(partitions[-partition])
test_indexes     = partitions[[partition]]
training_data    = beans_data[training_indexes,-17]
training_labels  = beans_data[training_indexes,17]
test_data        = beans_data[test_indexes,-17]
test_labels      = beans_data[test_indexes,17]
## Normalização por Score-Z
normalization_parameters = preProcess(training_data, method = c("center","scale"))
training_data            = predict(normalization_parameters, training_data)
test_data                = predict(normalization_parameters, test_data)
rm(normalization_parameters)
gmm.model = MclustDA(training_data, training_labels, modelNames = c('VII'), verbose = FALSE)
gmm.predict = predict(gmm.model, test_data)
confusion_matrix = confusionMatrix(gmm.predict$classification, test_labels)
print(confusion_matrix)
results[partition] = confusion_matrix$overall[1]
}
mean(results)
rm(list=ls())
library(caret)
library(mclust)
beans_data = read.csv("Dry_Bean_Dataset.csv",header=T, sep=";", na.strings="?", dec=",")
str(beans_data)
class_factor = factor(beans_data$Class)
beans_data$Class = class_factor
set.seed(1)
partitions = createFolds(beans_data$Class, k = 3)
results = c(0,0,0)
for(partition in 1:3) {
training_indexes = unlist(partitions[-partition])
test_indexes     = partitions[[partition]]
training_data    = beans_data[training_indexes,-17]
training_labels  = beans_data[training_indexes,17]
test_data        = beans_data[test_indexes,-17]
test_labels      = beans_data[test_indexes,17]
gmm.model = MclustDA(training_data, training_labels, modelNames = c('VII'), verbose = FALSE)
gmm.predcition = predict(gmm.model, test_data)
confusion_matrix = confusionMatrix(gmm.predict$classification, test_labels)
print(confusion_matrix)
results[partition] = confusion_matrix$overall[1]
}
mean(results)
rm(list=ls())
library(caret)
library(mclust)
beans_data = read.csv("Dry_Bean_Dataset.csv",header=T, sep=";", na.strings="?", dec=",")
str(beans_data)
class_factor = factor(beans_data$Class)
beans_data$Class = class_factor
set.seed(1)
partitions = createFolds(beans_data$Class, k = 3)
results = c(0,0,0)
for(partition in 1:3) {
training_indexes = unlist(partitions[-partition])
test_indexes     = partitions[[partition]]
training_data    = beans_data[training_indexes,-17]
training_labels  = beans_data[training_indexes,17]
test_data        = beans_data[test_indexes,-17]
test_labels      = beans_data[test_indexes,17]
gmm.model = MclustDA(training_data, training_labels, modelNames = c('VII'), verbose = FALSE)
gmm.predict = predict(gmm.model, test_data)
confusion_matrix = confusionMatrix(gmm.predict$classification, test_labels)
print(confusion_matrix)
results[partition] = confusion_matrix$overall[1]
}
mean(results)
rm(list=ls())
library(caret)
library(mclust)
beans_data = read.csv("Dry_Bean_Dataset.csv",header=T, sep=";", na.strings="?", dec=",")
str(beans_data)
class_factor = factor(beans_data$Class)
beans_data$Class = class_factor
set.seed(1)
partitions = createFolds(beans_data$Class, k = 3)
results = c(0,0,0)
for(partition in 1:3) {
training_indexes = unlist(partitions[-partition])
test_indexes     = partitions[[partition]]
training_data    = beans_data[training_indexes,-17]
training_labels  = beans_data[training_indexes,17]
test_data        = beans_data[test_indexes,-17]
test_labels      = beans_data[test_indexes,17]
## Normalização por Score-Z
normalization_parameters = preProcess(training_data, method = c("center","scale"))
training_data            = predict(normalization_parameters, training_data)
test_data                = predict(normalization_parameters, test_data)
rm(normalization_parameters)
## Projeção dos dados. Sem redução de dimensionalidade
training_pca  = preProcess(training_data,
method=c("center", "scale", "pca"), thresh = 1.0)
training_data = predict(training_pca, training_data)
test_pca  = preProcess(test_data,
method=c("center", "scale", "pca"), thresh = 1.0)
test_data = predict(test_pca, test_data)
## Treinamento de dados utilizando modelo de misturas gaussianas de covariância esférica
gmm.model = MclustDA(training_data, training_labels, modelNames = c("VII"), verbose = FALSE)
## Predição dos dados
gmm.predict = predict(gmm.model, test_data)
confusion_matrix = confusionMatrix(gmm.predict$classification, test_labels)
print(confusion_matrix)
results[partition] = confusion_matrix$overall[1]
}
mean(results)
rm(list=ls())
library(caret)
library(mclust)
beans_data = read.csv("Dry_Bean_Dataset.csv",header=T, sep=";", na.strings="?", dec=",")
str(beans_data)
class_factor = factor(beans_data$Class)
beans_data$Class = class_factor
set.seed(1)
partitions = createFolds(beans_data$Class, k = 3)
results = c(0,0,0)
for(partition in 1:3) {
training_indexes = unlist(partitions[-partition])
test_indexes     = partitions[[partition]]
training_data    = beans_data[training_indexes,-17]
training_labels  = beans_data[training_indexes,17]
test_data        = beans_data[test_indexes,-17]
test_labels      = beans_data[test_indexes,17]
## Normalização por Score-Z
normalization_parameters = preProcess(training_data, method = c("center","scale"))
training_data            = predict(normalization_parameters, training_data)
test_data                = predict(normalization_parameters, test_data)
rm(normalization_parameters)
## Projeção dos dados com redução de dimensionalidade. 98% de variância acumulada
training_pca  = preProcess(training_data,
method=c("center", "scale", "pca"), thresh = .98)
training_data = predict(training_pca, training_data)
test_pca  = preProcess(test_data,
method=c("center", "scale", "pca"), thresh = .98)
test_data = predict(test_pca, test_data)
## Treinamento de dados utilizando modelo de misturas gaussianas de covariância esférica
gmm.model = MclustDA(training_data, training_labels, modelNames = c("VII"), verbose = FALSE)
## Predição dos dados
gmm.predict = predict(gmm.model, test_data)
confusion_matrix = confusionMatrix(gmm.predict$classification, test_labels)
print(confusion_matrix)
results[partition] = confusion_matrix$overall[1]
}
mean(results)
View(gmm.model)
View(gmm.model)
View(gmm.model)
mapply(nMclustParameters, nMclustParams, mclust.options("emModelNames"), d = 6, G = 5)
mapply(nMclustParams, mclust.options("emModelNames"), d = 6, G = 5)
rm(list=ls()) #Limpeza do Ambiente
require(cluster)
require(stats)
require(factoextra)
require(gridExtra)
rm(list=ls()) #Limpeza do Ambiente
data = read.csv('Mall_Customers.csv') # Carregar o dataset
