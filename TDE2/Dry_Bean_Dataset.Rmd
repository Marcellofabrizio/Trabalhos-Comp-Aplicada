---
title: "Tarefa 2 - Preparação de Dados"
author: "Leonardo Martelli e Marcello Fabrizio"
date: "24/10/2021"
output:
  html_document:
    df_print: paged
  pdf_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### 0 - Limpeza do ambiente e carregamento de pacotes0

```{r packs, warning=FALSE, message = FALSE}
rm(list=ls())

library('ggplot2')
library('gridExtra')
library('GGally')
library(grid)
library(stringr)
library(corrplot)
library(caret)
library(ggfortify)
library(qcc)
```

### 1 - Carregue a base de dados e mostre a estrutura do dataset (str()). O arquivo do dataset não pode ser modificado de forma alguma. A leitura deverá tratar qualquer característica do arquivo.

```{r load data, warning=FALSE, message=FALSE}
beans_data = read.csv("Dry_Bean_Dataset.csv",header=T, sep=";", na.strings="?", dec=",")
str(beans_data)
```

### 2 - Altere a variável do tipo do feijão (Class) para um factor. 
```{r factor, warning=FALSE, message=FALSE}
class_factor = factor(beans_data$Class)
beans_data$Class = class_factor
```

### 3 - Plote um gráfico de barras que ilustre as quantidades de cada classe.
```{r bar plot, warning=FALSE, message=FALSE}
ggplot(beans_data, aes(Class, fill = Class)) + geom_bar() + theme(legend.position = "none")
```

### 4 - Realize a normalização dos dados via Z-score. Plote um boxplot para ilustrar a distribuição de cada variável. Mostre as estatísticas de cada variável (summary). 
```{r normalization, warning=FALSE, message=FALSE}


for (var in names(beans_data[1:16])) {
  mean    = mean(beans_data[[var]])
  sd      = sd(beans_data[[var]])
  z_score = (beans_data[[var]] - mean)/sd
  
  beans_data[[var]] = z_score
}

#scale obtem o mesmo resultado do que acima
#z_score = as.data.frame(scale(beans_data[,1:16]))

summary(beans_data)

classes = unique(beans_data$Class)

num_classes = length(classes)

colors=c("red", "green", "blue")

par(mfrow=c(1,3), cex=0.5)
for(i in 1:3){
  plot <- boxplot(beans_data[,i] ~ beans_data$Class, 
                  col=colors, xlab = "", 
                  ylab="", 
                  main = names(beans_data)[i], 
                  xaxt = "n",  
                  yaxt = "n")
  axis(side = 1, labels = FALSE)
  axis(side = 2, las = 2, cex.axis = 0.9, font =20)
  text(x = 1:num_classes,
       labels = classes,
       par("usr")[3],
       xpd = T,
       srt = 90,
       adj = 1.2,
       cex = 0.9)
}

par(mfrow=c(1,3), cex=0.5)
for(i in 4:6){
  plot <- boxplot(beans_data[,i] ~ beans_data$Class, 
                  col=colors, xlab = "", 
                  ylab="", 
                  main = names(beans_data)[i], 
                  xaxt = "n",  
                  yaxt = "n")
  axis(side = 1, labels = FALSE)
  axis(side = 2, las = 2, cex.axis = 0.9, font =20)
  text(x = 1:num_classes,
       labels = classes,
       par("usr")[3],
       xpd = T,
       srt = 90,
       adj = 1.2,
       cex = 0.9)
}

par(mfrow=c(1,3), cex=0.5)
for(i in 7:9){
  plot <- boxplot(beans_data[,i] ~ beans_data$Class, 
                  col=colors, xlab = "", 
                  ylab="", 
                  main = names(beans_data)[i], 
                  xaxt = "n",  
                  yaxt = "n")
  axis(side = 1, labels = FALSE)
  axis(side = 2, las = 2, cex.axis = 0.9, font =20)
  text(x = 1:num_classes,
       labels = classes,
       par("usr")[3],
       xpd = T,
       srt = 90,
       adj = 1.2,
       cex = 0.9)
}

par(mfrow=c(1,3), cex=0.5)
for(i in 10:12){
  plot <- boxplot(beans_data[,i] ~ beans_data$Class, 
                  col=colors, xlab = "", 
                  ylab="", 
                  main = names(beans_data)[i], 
                  xaxt = "n",  
                  yaxt = "n")
  axis(side = 1, labels = FALSE)
  axis(side = 2, las = 2, cex.axis = 0.9, font =20)
  text(x = 1:num_classes,
       labels = classes,
       par("usr")[3],
       xpd = T,
       srt = 90,
       adj = 1.2,
       cex = 0.9)
}

par(mfrow=c(1,3), cex=0.5)
for(i in 13:15){
  plot <- boxplot(beans_data[,i] ~ beans_data$Class, 
                  col=colors, xlab = "", 
                  ylab="", 
                  main = names(beans_data)[i], 
                  xaxt = "n",  
                  yaxt = "n")
  axis(side = 1, labels = FALSE)
  axis(side = 2, las = 2, cex.axis = 0.9, font =20)
  text(x = 1:num_classes,
       labels = classes,
       par("usr")[3],
       xpd = T,
       srt = 90,
       adj = 1.2,
       cex = 0.9)
}

par(mfrow=c(1,3), cex=0.5)
for(i in 16:16){
  plot <- boxplot(beans_data[,i] ~ beans_data$Class, 
                  col=colors, xlab = "", 
                  ylab="", 
                  main = names(beans_data)[i], 
                  xaxt = "n",  
                  yaxt = "n")
  axis(side = 1, labels = FALSE)
  axis(side = 2, las = 2, cex.axis = 0.9, font =20)
  text(x = 1:num_classes,
       labels = classes,
       par("usr")[3],
       xpd = T,
       srt = 90,
       adj = 1.2,
       cex = 0.9)
}
```

### 5 - Realize a seleção de características (correlação). Plote o gráfico de correlação. Liste as características que foram removidas. 
```{r correlation matrix, message=FALSE, warning=FALSE}

correlation_matrix = cor(beans_data[1:16])

strong_correlations = findCorrelation(correlation_matrix, cutoff=0.95)

if(length(strong_correlations) > 0) {
  beans_data[,strong_correlations] = NULL
}

corrplot(correlation_matrix)

summary(beans_data)
```
Foi utilizada a função findCorrelation para obter os indices que denotam as colunas a serem removidas. Ela os obtem examinando a correlação média de cada variável em um par de variáveis, e remove a variável com correlação acima da taxa de corte. Foram removidas as variáveis com correlção maior que 0.95, ou seja, aquelas que possuiam uma alta correlação. Em alguns casos, variáveis com alta correlação acabam por não fornecer muita informação sobre o conjunto de dados estudados, ou seja, um par de varíaveis pode estar fornecendo a mesma informação. Entretanto, devemos tomar cuidado ao remover varíaveis, pois correlação não implica causalidade. De forma geral isso pode ser visto como um caso da Navalha de Occam, onde o modelo mais simples, ou seja, com menos variáveis, é preferido.  

### 6 - Plote um gráfico boxplot ou de densidade por variável x classe (organize em 3 colunas). Discuta qual é a variável que teria maior poder de discriminação? Existe alguma classe que pode ser classificada mais facilmente? Justifique a sua escolha.

```{r boxplots, message=FALSE, warning=FALSE}
library(gridExtra)
library(ggplot2)

p = list()
for(i in 1:3){
  p[[i]] = ggplot(beans_data, aes_string(x=names(beans_data)[i],fill="Class")) + 
    geom_density(alpha=0.5,color="darkgray") + 
    theme(legend.position="none", 
          legend.title = element_blank())
}
do.call(grid.arrange,c(p,ncol=3))

p = list()
for(i in 4:6){
  p[[i - 3]] = ggplot(beans_data, aes_string(x=names(beans_data)[i],fill="Class")) + 
    geom_density(alpha=0.5,color="darkgray") + 
    theme(legend.position="none", 
          legend.title = element_blank())
}

do.call(grid.arrange,c(p,ncol=3))

p = list()
for(i in 7:9){
  p[[i - 6]] = ggplot(beans_data, aes_string(x=names(beans_data)[i],fill="Class")) + 
    geom_density(alpha=0.5,color="darkgray") + 
    theme(legend.position="none", 
          legend.title = element_blank())
}
do.call(grid.arrange, c(p,ncol=3))

p = list()
for(i in 10){
  p[[i-9]] = ggplot(beans_data, aes_string(x=names(beans_data)[i],fill="Class")) + 
    geom_density(alpha=0.5,color="darkgray") + 
    theme(legend.position="none", 
          legend.title = element_blank())
}

do.call(grid.arrange,c(p,ncol=1))
```
Analizando os resultados, percebemos que a variável ShapeFactor2 possui um alto poder de discriminação das classes, pois podemos notar a separação das médias das densidades. Essa discriminação também é vista nas variáveis roudness e Area, porem em um nível menor se comparadas a ShapeFactor2.

### 7 - Realize a projeção do dataset utilizando PCA. Explique as características dos componentes principais estimados. O que se pode explicar sobre os componentes principais utilizando o gráfico biplot. Apresente as características básicas (summary) dos dados.

```{r PCA, warning=FALSE, message=FALSE}

covariance = cov(scale(beans_data[,1:9]))
eig = eigen(covariance)  

eigen_values  = eig$values
eigen_vectors = eig$vectors

print(eig)

barplot(eig$values,ylab  = "Variância",xlab = "Componente Principal",names.arg =1:9)

pca = prcomp(beans_data[,1:9], center=TRUE, scale=TRUE)

pca$sdev

explained_variance = pca$sdev^2 / sum(pca$sdev^2)

biplot(pca,xlabs = rep("", nrow(beans_data)))

pareto.chart(
  explained_variance,
  x = c(1:9)  
)

```
Analaziando os valores da matriz de valores dos PCAs, nota-se que PC1 afeta diretamente as varíaveis Area, AspectRation, Eccentricity