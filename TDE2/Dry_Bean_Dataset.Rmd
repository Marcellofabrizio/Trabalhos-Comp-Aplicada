---
title: "Tarefa 2 - Preparação de Dados"
author: "Leonardo Martelli e Marcello Fabrizio"
date: "24/10/2021"
output:
  html_document:
    df_print: paged
  pdf_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### 0 - Limpeza do ambiente e carregamento de pacotes0

```{r packs, warning=FALSE, message = FALSE}
rm(list=ls())

library('ggplot2')
library('gridExtra')
library('GGally')
library(grid)
library(stringr)
library(corrplot)
library(caret)
library(ggfortify)
```

### 1 - Carregue a base de dados e mostre a estrutura do dataset (str()). O arquivo do dataset não pode ser modificado de forma alguma. A leitura deverá tratar qualquer característica do arquivo.

```{r load data, warning=FALSE, message=FALSE}
beans_data = read.csv("Dry_Bean_Dataset.csv",header=T, sep=";", na.strings="?", dec=",")
str(beans_data)
```

### 2 - Altere a variável do tipo do feijão (Class) para um factor. 
```{r factor, warning=FALSE, message=FALSE}
class_factor = factor(beans_data$Class)
beans_data$Class = class_factor
```

### 3 - Plote um gráfico de barras que ilustre as quantidades de cada classe.
```{r bar plot, warning=FALSE, message=FALSE}
ggplot(beans_data, aes(Class, fill = Class)) + geom_bar() + theme(legend.position = "none")
```

### 4 - Realize a normalização dos dados via Z-score. Plote um boxplot para ilustrar a distribuição de cada variável. Mostre as estatísticas de cada variável (summary). 
```{r normalization, warning=FALSE, message=FALSE}

normalized_data = preProcess(beans_data, method=c("center", "scale"))

beans_data = predict(normalized_data, beans_data)
summary(beans_data)

classes = unique(beans_data$Class)
num_classes = length(classes)

colors=c("red", "green", "blue")

par(mfrow=c(1,3), cex=0.5)
for(i in 1:3){
  plot <- boxplot(beans_data[,i] ~ beans_data$Class, 
                  col=colors, xlab = "", 
                  ylab="", 
                  main = names(beans_data)[i], 
                  xaxt = "n",  
                  yaxt = "n")
  axis(side = 1, labels = FALSE)
  axis(side = 2, las = 2, cex.axis = 0.9, font =20)
  text(x = 1:num_classes,
       labels = classes,
       par("usr")[3],
       xpd = T,
       srt = 90,
       adj = 1.2,
       cex = 0.9)
}

par(mfrow=c(1,3), cex=0.5)
for(i in 4:6){
  plot <- boxplot(beans_data[,i] ~ beans_data$Class, 
                  col=colors, xlab = "", 
                  ylab="", 
                  main = names(beans_data)[i], 
                  xaxt = "n",  
                  yaxt = "n")
  axis(side = 1, labels = FALSE)
  axis(side = 2, las = 2, cex.axis = 0.9, font =20)
  text(x = 1:num_classes,
       labels = classes,
       par("usr")[3],
       xpd = T,
       srt = 90,
       adj = 1.2,
       cex = 0.9)
}

par(mfrow=c(1,3), cex=0.5)
for(i in 7:9){
  plot <- boxplot(beans_data[,i] ~ beans_data$Class, 
                  col=colors, xlab = "", 
                  ylab="", 
                  main = names(beans_data)[i], 
                  xaxt = "n",  
                  yaxt = "n")
  axis(side = 1, labels = FALSE)
  axis(side = 2, las = 2, cex.axis = 0.9, font =20)
  text(x = 1:num_classes,
       labels = classes,
       par("usr")[3],
       xpd = T,
       srt = 90,
       adj = 1.2,
       cex = 0.9)
}

par(mfrow=c(1,3), cex=0.5)
for(i in 10:12){
  plot <- boxplot(beans_data[,i] ~ beans_data$Class, 
                  col=colors, xlab = "", 
                  ylab="", 
                  main = names(beans_data)[i], 
                  xaxt = "n",  
                  yaxt = "n")
  axis(side = 1, labels = FALSE)
  axis(side = 2, las = 2, cex.axis = 0.9, font =20)
  text(x = 1:num_classes,
       labels = classes,
       par("usr")[3],
       xpd = T,
       srt = 90,
       adj = 1.2,
       cex = 0.9)
}

par(mfrow=c(1,3), cex=0.5)
for(i in 13:15){
  plot <- boxplot(beans_data[,i] ~ beans_data$Class, 
                  col=colors, xlab = "", 
                  ylab="", 
                  main = names(beans_data)[i], 
                  xaxt = "n",  
                  yaxt = "n")
  axis(side = 1, labels = FALSE)
  axis(side = 2, las = 2, cex.axis = 0.9, font =20)
  text(x = 1:num_classes,
       labels = classes,
       par("usr")[3],
       xpd = T,
       srt = 90,
       adj = 1.2,
       cex = 0.9)
}

par(mfrow=c(1,3), cex=0.5)
for(i in 16:16){
  plot <- boxplot(beans_data[,i] ~ beans_data$Class, 
                  col=colors, xlab = "", 
                  ylab="", 
                  main = names(beans_data)[i], 
                  xaxt = "n",  
                  yaxt = "n")
  axis(side = 1, labels = FALSE)
  axis(side = 2, las = 2, cex.axis = 0.9, font =20)
  text(x = 1:num_classes,
       labels = classes,
       par("usr")[3],
       xpd = T,
       srt = 90,
       adj = 1.2,
       cex = 0.9)
}
```

### 5 - Realize a seleção de características (correlação). Plote o gráfico de correlação. Liste as características que foram removidas. 
```{r correlation matrix, message=FALSE, warning=FALSE}

correlation_matrix = cor(beans_data[1:16])

strong_correlations = findCorrelation(correlation_matrix, cutoff=0.95)

#Variáveis que serão removidas devido a sua alta correlação
names(beans_data[,strong_correlations])

if(length(strong_correlations) > 0) {
  beans_data[,strong_correlations] = NULL
}

corrplot(correlation_matrix)

summary(beans_data)
```
Foi utilizada a função findCorrelation para obter os indices que denotam as colunas a serem removidas. Ela os obtem examinando a correlação média de cada variável em um par de variáveis, e remove a variável com correlação acima da taxa de corte. Foram removidas as variáveis com correlção maior que 0.95, ou seja, aquelas que possuiam uma alta correlação. Em alguns casos, variáveis com alta correlação acabam por não fornecer muita informação sobre o conjunto de dados estudados, ou seja, um par de varíaveis pode estar fornecendo a mesma informação. Entretanto, devemos tomar cuidado ao remover varíaveis, pois correlação não implica causalidade. De forma geral isso pode ser visto como um caso da Navalha de Occam, onde o modelo mais simples, ou seja, com menos variáveis, é preferido.  

### 6 - Plote um gráfico boxplot ou de densidade por variável x classe (organize em 3 colunas). Discuta qual é a variável que teria maior poder de discriminação? Existe alguma classe que pode ser classificada mais facilmente? Justifique a sua escolha.

```{r boxplots, message=FALSE, warning=FALSE}
library(gridExtra)
library(ggplot2)

p = list()
for(i in 1:3){
  p[[i]] = ggplot(beans_data, aes_string(x=names(beans_data)[i],fill="Class")) + 
    geom_density(alpha=0.5,color="darkgray") + 
    theme(legend.position="none", 
          legend.title = element_blank())
}
do.call(grid.arrange,c(p,ncol=3))

p = list()
for(i in 4:6){
  p[[i - 3]] = ggplot(beans_data, aes_string(x=names(beans_data)[i],fill="Class")) + 
    geom_density(alpha=0.5,color="darkgray") + 
    theme(legend.position="none", 
          legend.title = element_blank())
}

do.call(grid.arrange,c(p,ncol=3))

p = list()
for(i in 7:9){
  p[[i - 6]] = ggplot(beans_data, aes_string(x=names(beans_data)[i],fill="Class")) + 
    geom_density(alpha=0.5,color="darkgray") + 
    theme(legend.position="none", 
          legend.title = element_blank())
}
do.call(grid.arrange, c(p,ncol=3))

p = list()
for(i in 10){
  p[[i-9]] = ggplot(beans_data, aes_string(x=names(beans_data)[i],fill="Class")) + 
    geom_density(alpha=0.5,color="darkgray") + 
    theme(legend.position="none", 
          legend.title = element_blank())
}

do.call(grid.arrange,c(p,ncol=1))
```
Analizando os resultados, percebemos que a variável ShapeFactor2 possui um alto poder de discriminação das classes, pois podemos notar a separação das médias das densidades. Essa discriminação também é vista nas variáveis roudness e Area, porem em um nível menor se comparadas a ShapeFactor2.

### 7 - Realize a projeção do dataset utilizando PCA. Explique as características dos componentes principais estimados. O que se pode explicar sobre os componentes principais utilizando o gráfico biplot. Apresente as características básicas (summary) dos dados.

```{r PCA, warning=FALSE, message=FALSE}

pca = prcomp(beans_data[,1:9], center=TRUE, scale=TRUE)

print(pca)
summary(pca)

explained_variance = pca$sdev^2 / sum(pca$sdev^2)

biplot(pca,xlabs = rep("", nrow(beans_data)))

barplot (
  explained_variance, 
  names.arg=1:9, 
  main = "Variância",
  xlab = "Componentes Principais",
  ylab = "Variância Explicada",
  col ="steelblue"
)

lines (
  x = 1:9, 
  explained_variance, 
  type="b", 
  pch=19, 
  col = "red"
)

cumpro <- cumsum(pca$sdev^2 / sum(pca$sdev^2))
plot(cumpro[1:9], xlab = "Componentes Principais", ylab = "Variância Explicada", main = "Gŕafico de Variância")
```

Interpretando o gráfico biplot resultante e as variâncias das variáveis, temos que as variáveis Area, AspectRatio e Eccentricity influenciam positivamente a proporção da variância de PC1, enquanto as demais afetam negativamente. Podemos visualizar isso analisando os vetores do gráfico biplot, onde notamos que as variáveis que afetam positivamente estão alinhadas com o eixo de PC1, apontando em direção positiva, enquanto as que afetam negativamente estão apontando para a direção negativa. Para PC2, temos que as variáveis Area, Extent, Solidity e roundness a afetam positivamente. 

Nota-se que a variável Area afeta positivamente ambos PC1 e PC2. Podemos ver essa relação no gráfico, onde Area aponta em direção positva a PC1 e PC2.

Temos também que PC1 explica 50% da varíancia do conjunto.

### 8 - Analise o dataset projetado com o auxílio do gráfico de boxplot por classe (igual ao do item 6).  Compare com o resultado do item 6. Se quiser, pode gerar um gráfico de espalhamento para auxiliar na explicação. 

```{r comparing, warning=FALSE, message=FALSE}
#thresh = 1.0 não reduz dimensionalidade
pca_params = preProcess(beans_data, method=c("center", "scale", "pca"), thresh = 1.0)

projected_data = predict(pca_params, beans_data)
summary(projected_data)

names(projected_data)

colors=c("red", "green", "blue")

par(mfrow=c(1,3), cex=0.5)
for(i in 2:4){
  plot <- boxplot(projected_data[,i] ~ projected_data$Class, 
                  col=colors, xlab = "", 
                  ylab="", 
                  main = names(projected_data)[i], 
                  xaxt = "n",  
                  yaxt = "n")
  axis(side = 1, labels = FALSE)
  axis(side = 2, las = 2, cex.axis = 0.9, font =20)
  text(x = 1:num_classes,
       labels = classes,
       par("usr")[3],
       xpd = T,
       srt = 90,
       adj = 1.2,
       cex = 0.9)
}

par(mfrow=c(1,3), cex=0.5)
for(i in 5:7){
  plot <- boxplot(projected_data[,i] ~ projected_data$Class, 
                  col=colors, xlab = "", 
                  ylab="", 
                  main = names(projected_data)[i], 
                  xaxt = "n",  
                  yaxt = "n")
  axis(side = 1, labels = FALSE)
  axis(side = 2, las = 2, cex.axis = 0.9, font =20)
  text(x = 1:num_classes,
       labels = classes,
       par("usr")[3],
       xpd = T,
       srt = 90,
       adj = 1.2,
       cex = 0.9)
}

par(mfrow=c(1,2), cex=0.5)
for(i in 8:9){
  plot <- boxplot(projected_data[,i] ~ projected_data$Class, 
                  col=colors, xlab = "", 
                  ylab="", 
                  main = names(projected_data)[i], 
                  xaxt = "n",  
                  yaxt = "n")
  axis(side = 1, labels = FALSE)
  axis(side = 2, las = 2, cex.axis = 0.9, font =20)
  text(x = 1:num_classes,
       labels = classes,
       par("usr")[3],
       xpd = T,
       srt = 90,
       adj = 1.2,
       cex = 0.9)
}
```

### 9 - É possível reduzir a dimensionalidade dos dados? Explique como! 

Sim, é possível reduzir a dimensionalidade dos dados. Podemos fazer isso durante a transformação dos dados com a função preProcess, estipulando um valor de corte para a porcentagem cumulativa de variância da componente principal. Ou seja, serão eliminadas todas as dimensões dos dados transformados com porcentagem de variância cumulativa acima do valor de corte.
```{r reduction, warning=FALSE, message=FALSE}
reduced_pca_params = preProcess(beans_data, method=c("center", "scale", "pca"), thresh = .95)

reduced_projected_data = predict(reduced_pca_params, beans_data)
summary(reduced_projected_data)
```
Aqui utilizamos a porcentagem de 95% da variância cumulativa como corte e tivemos uma redução de três dimensões do conjunto de dados transfomado.

### 10 - Analise o dataset reduzido com o auxílio do gráfico de boxplot por classe (igual ao do item 6). .  Compare com o resultado do item 6 e do item 8. Se quiser, pode gerar um gráfico de espalhamento para auxiliar na explicação.